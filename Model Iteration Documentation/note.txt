Iteration 1:
- normalization using min 2200 and max 4100
- normalize imu by dividing using int 20
- total row for each label isn't equal
- model: LSTM 64, LSTM 128, LSTM 64, Dense 64, Dense 32, Dense 26
- initial LR 5 x 10e-4
- batch size 32
- epoch 150
- LR reducer: factor 0.8, patience 7, min LR 1 x 10e-6

Iteration 2:
- - normalization using min 2200 and max 4100
- normalize imu by dividing using int 20
- total row for each label isn't equal
- model: LSTM 64, LSTM 128, Dense 64, Dense 26
- initial LR 5 x 10e-4
- batch size 32
- epoch 150
- LR reducer: factor 0.8, patience 7, min LR 1 x 10e-6

Iteration 3:
- LR reducer: factor 0.5, patience 5, min LR 1 x 10e-6

Iteration 4:
- use pytorch
- epoch 200, learning rate 1e-4, patience 5
- use lr scheduler
- Adam optimizer
- Data sequence length 20
- Model use 2 layers stacked lstm, lstm dropout 0.3, hidden size 64, input size 5, batch normalization, flatten layer (seq length * hidden size)
- batch size 32
- one label for one sequential data (20 row, 1 one hot encoded label)
- test accuracy only +-50%

Iteration 4:
- use pytorch
- bidirectional lstm
- 1 fc (as output layer)
- add SoftMax at final layer

Iteration 5:
- use pytorch
- bidirectional lstm
- train size 65, val size 17.5, test size 17.5
- 1 fc output layer, flatten layer, no SoftMax at final layer
- lstm stack : 2

Iteration 6:
- use pytorch
- bidirectional lstm
- train size 60, val size 20, test size 20
- 1 fc output layer, flatten layer, batch normalization layer, relu act function. no SoftMax at final layer
- lstm stack : 3
- 2 fully connected layer
- normalization min: 2500, max: 4100

Iteration 7:
- remove batch normalization layer, use layer normalization instead
- validation accuracy 95,48

Iteration 8:
- lstm 3 layers, bidirectional, use last output only
- flex imu val accuracy : 97.49
- flex val accuracy : 92.07

Iteration 9:
- fixed test score function
- lstm 3 layers, bidirectional, use last output only, dropout layer
- only use data from zalfa, anes, ade, alfa
- train size 0.8, test size 0.1, val size 0.1
- sequence window stride: 2
- flex val accuracy: 94.55 ; test accuracy: 95
- flex imu val accuracy: 98.4 ; test accuracy: 98

Iteration 10:
- use all data except anis
- flex val accuracy: 97.06 ; test accuracy: 97
- flex imu val accuracy: 99.84 ; test accuracy: 99

Iteration 11:
- use all data except anis
- train size 70%
- use flatten layer
- use only 20 last row
- flex val accuracy: 97.06 ; test accuracy: 97
- flex imu val accuracy: 99.84 ; test accuracy: 99

Iteration 12:
- use only data from ade, alfa, zalfa, timo, nurhadi, bayu, anes

Iteration 13:
- use only data from nurhadi, amalia, bayu, ade

Iteration 14:
- use new synthetic data

Iteration 15:
- adjustment on synthetic data
- not use data from alfa and ade

Iteration 16:
- adjustment on synthetic data
- remove data from alfa, ade, zalfa, anes

Iteration 17:
- adjustment on synthetic data

Iteration 18:
- adjustment on synthetic data